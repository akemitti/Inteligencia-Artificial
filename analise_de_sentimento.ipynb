{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca6ce41",
   "metadata": {},
   "source": [
    "# 1) Extraindo os dados dos arquivos PDF\n",
    "> Seção introdutória (markdown) que anuncia a etapa de extração de textos a partir de PDFs.\n",
    "\n",
    "---\n",
    "\n",
    "## Bloco 1 — Instalações de dependências\n",
    "**O que faz:** instala bibliotecas para leitura de PDF:\n",
    "\n",
    "- pypdf — parser de PDFs (lê estrutura, páginas).\n",
    "\n",
    "- pdfminer.six — extrator de texto de PDF (converte PDF→texto).\n",
    "\n",
    "**Por quê:** garantir ambiente reprodutível com versões fixadas (evita “quebras” ao longo do tempo).\n",
    "\n",
    "**Saída:** nenhuma variável; apenas instala pacotes no ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af753c51",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf==4.3.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.3.1)\n",
      "Requirement already satisfied: pdfminer.six==20240706 in /usr/local/python/3.12.1/lib/python3.12/site-packages (20240706)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from pdfminer.six==20240706) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pdfminer.six==20240706) (46.0.3)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six==20240706) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /home/codespace/.local/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20240706) (2.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf==4.3.1 pdfminer.six==20240706"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a437e6",
   "metadata": {},
   "source": [
    "## Bloco 2 — Varredura e cache de parágrafos por pasta/ano\n",
    "\n",
    "**O que faz:** percorre a **raiz** com PDFs, filtra por **anos** desejados, extrai parágrafos de cada documento (via `processar_pdf`) e salva/recupera **cache** em `*.pkl` (arquivo binário do pandas) para acelerar novas execuções.\n",
    "\n",
    "**Parâmetros (principais):**\n",
    "\n",
    "* `raiz` (caminho da pasta): diretório onde estão os PDFs.\n",
    "* `min_palavras` (limiar de qualidade): descarta/funde parágrafos muito curtos.\n",
    "* `anos` (filtro temporal): limita os PDFs pelos anos no nome do arquivo (padrão `{2022, 2023, 2024, 2025}`).\n",
    "* `cache_dir` (cache em disco): pasta dos `*.pkl`.\n",
    "* `rebuild_cache` (reprocessar?): se `True`, ignora cache e reconstrói.\n",
    "\n",
    "**Conceitos rápidos:**\n",
    "\n",
    "* **Cache**: armazenamento intermediário de resultados para evitar reprocessar tudo a cada execução.\n",
    "* **YYYYMM**: carimbo de ano+mês (ex.: `202312`).\n",
    "\n",
    "**Saída:** `DataFrame` com colunas `[\"doc\",\"dt\",\"pid\",\"texto\",\"n_palavras\"]`, ordenado.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d3537a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_raiz(raiz: str,\n",
    "                   min_palavras: int = 8,\n",
    "                   anos: set[int] | None = {2022, 2023, 2024, 2025},\n",
    "                   cache_dir: str = \"out/estatisticas_paragrafos\",\n",
    "                   rebuild_cache: bool = False) -> pd.DataFrame:\n",
    "    raiz = Path(raiz)\n",
    "    cache = Path(cache_dir)\n",
    "    cache.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dfs, vistos = [], set()\n",
    "    # 1) varredura recursiva em subpastas yyyy\n",
    "    for pdf in sorted(raiz.rglob(\"*.pdf\")):\n",
    "        name = pdf.stem  # ex.: \"202212_Texto_de_estatisticas_monetarias_e_de_credito\"\n",
    "        # 2) captura robusta do YYYYMM onde quer que esteja no nome\n",
    "        m = re.search(r'(?<!\\d)(\\d{6})(?!\\d)', name)\n",
    "        if not m:\n",
    "            continue\n",
    "        yyyymm = m.group(1)\n",
    "        ano = int(yyyymm[:4])\n",
    "        mes = int(yyyymm[4:6])\n",
    "\n",
    "        # limita por anos desejados\n",
    "        if anos is not None and ano not in anos:\n",
    "            continue\n",
    "        # evita duplicatas (mesmo YYYYMM em mais de um arquivo)\n",
    "        if yyyymm in vistos:\n",
    "            continue\n",
    "\n",
    "        # 3) cache .pkl por documento\n",
    "        pkl_path = cache / f\"{yyyymm}.pkl\"\n",
    "        if (not rebuild_cache) and pkl_path.exists():\n",
    "            try:\n",
    "                df = pd.read_pickle(pkl_path)\n",
    "            except Exception:\n",
    "                df = processar_pdf(pdf, min_palavras=min_palavras)\n",
    "                df.to_pickle(pkl_path)\n",
    "        else:\n",
    "            df = processar_pdf(pdf, min_palavras=min_palavras)\n",
    "            df.to_pickle(pkl_path)\n",
    "\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "            vistos.add(yyyymm)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=[\"doc\",\"dt\",\"pid\",\"texto\",\"n_palavras\"])\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "    return out.sort_values([\"dt\",\"doc\",\"pid\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85347f6a",
   "metadata": {},
   "source": [
    "**O que faz:**\n",
    "\n",
    "1. Executa a coleta/extração de parágrafos e guarda em `df_paragrafos`.\n",
    "2. Exporta para `paragrafos.csv`.\n",
    "\n",
    "**Por quê:** persistir em CSV facilita auditoria e uso em outras ferramentas.\n",
    "\n",
    "**Saída:** arquivo `paragrafos.csv` no diretório atual.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f80a4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragrafos = processar_raiz(\"data/estatisticas\",\n",
    "                               min_palavras=8,\n",
    "                               anos={2022, 2023, 2024, 2025},\n",
    "                               rebuild_cache=True)    # 1ª rodada: gera o cache de todos\n",
    "df_paragrafos\n",
    "\n",
    "df_paragrafos.to_csv(\n",
    "    \"paragrafos.csv\",\n",
    "    index=False,          \n",
    "    header=True,          \n",
    "    na_rep=\"\",            # como representar valores ausentes (NaN)\n",
    "    float_format=\"%.2f\",  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2599c",
   "metadata": {},
   "source": [
    "# 2.1) Limpando títulos desnecessários\n",
    "\n",
    "> Seção (markdown) que introduz a **limpeza textual**: remoção de cabeçalhos/legendas, fusão de parágrafos curtos etc.\n",
    "\n",
    "---\n",
    "## Bloco 4 — Funções utilitárias de limpeza e fusão\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3177f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- helpers ---\n",
    "WORD_RE = re.compile(r\"\\b\\w+\\b\", flags=re.UNICODE)\n",
    "\n",
    "def count_words(s: str) -> int:\n",
    "    return len(WORD_RE.findall(s or \"\"))\n",
    "\n",
    "def caps_ratio(s: str) -> float:\n",
    "    toks = re.findall(r\"\\b[^\\W\\d_]+\\b\", s or \"\", flags=re.UNICODE)\n",
    "    if not toks: \n",
    "        return 0.0\n",
    "    return sum(t.isupper() for t in toks) / len(toks)\n",
    "\n",
    "_HEADING_PREFIXES = (\n",
    "    \"nota para a imprensa\",\n",
    "    \"sumário\",\"sumario\",\"resumo\",\n",
    "    \"apresentação\",\"apresentacao\",\n",
    "    \"introdução\",\"introducao\",\n",
    "    \"conclusão\",\"conclusoes\",\"conclusao\",\n",
    ")\n",
    "_CAPTION_HINTS = (\"figura\",\"gráfico\",\"grafico\",\"tabela\",\"fonte:\")\n",
    "\n",
    "def is_heading(text: str) -> bool:\n",
    "    s = (text or \"\").strip()\n",
    "    if not s:\n",
    "        return True\n",
    "    # \"2.\" ou \"2.3.\" no início\n",
    "    if re.match(r\"^\\d+(\\.\\d+)*\\s\", s):\n",
    "        return True\n",
    "    low = s.lower()\n",
    "    if low.startswith(_HEADING_PREFIXES):\n",
    "        return True\n",
    "    if any(k in low for k in _CAPTION_HINTS):\n",
    "        return True\n",
    "    # muito MAIÚSCULO (título), curto e sem pontuação final\n",
    "    if caps_ratio(s) > 0.6 and len(s.split()) <= 20:\n",
    "        return True\n",
    "    if len(s.split()) < 15 and not re.search(r\"[.!?;:]\\s*$\", s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clean_text_unit(s: str) -> str:\n",
    "    s = (s or \"\").replace(\"\\r\",\"\")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s).strip()\n",
    "    # remove numeração de seção no início\n",
    "    s = re.sub(r\"^\\d+(\\.\\d+)*\\s+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def _fuse_short_paragraphs(df_group: pd.DataFrame, min_words: int = 8, max_merge_span: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"Funde parágrafos consecutivos curtos até atingir min_words (no máx. 3 blocos por fusão).\"\"\"\n",
    "    rows, buf, buf_words, buf_span = [], [], 0, 0\n",
    "    for _, r in df_group.sort_values(\"pid\").iterrows():\n",
    "        txt = str(r[\"texto\"])\n",
    "        w = count_words(txt)\n",
    "        if w >= min_words:\n",
    "            if buf and buf_words >= min_words:\n",
    "                rows.append({\"texto\": \" \".join(buf)})\n",
    "            buf, buf_words, buf_span = [], 0, 0\n",
    "            rows.append({\"texto\": txt})\n",
    "        else:\n",
    "            buf.append(txt); buf_words += w; buf_span += 1\n",
    "            if buf_words >= min_words or buf_span >= max_merge_span:\n",
    "                rows.append({\"texto\": \" \".join(buf)})\n",
    "                buf, buf_words, buf_span = [], 0, 0\n",
    "    if buf and buf_words >= min_words:\n",
    "        rows.append({\"texto\": \" \".join(buf)})\n",
    "    out = pd.DataFrame(rows)\n",
    "    out[\"pid\"] = np.arange(1, len(out)+1, dtype=int)\n",
    "    return out\n",
    "\n",
    "def limpar_paragrafos(df: pd.DataFrame, min_palavras: int = 8, max_merge_span: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Entrada: df_paragrafos com colunas ['doc','dt','pid','texto','n_palavras'].\n",
    "    Saída: df_limpo com mesmas colunas, mas:\n",
    "      - headings/legendas removidos\n",
    "      - parágrafos curtos fundidos\n",
    "      - mínimo de 8 palavras garantido\n",
    "      - pid reindexado por (doc, dt)\n",
    "    \"\"\"\n",
    "    # checagem leve\n",
    "    for c in [\"doc\",\"dt\",\"pid\",\"texto\",\"n_palavras\"]:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Coluna obrigatória ausente: {c}\")\n",
    "\n",
    "    dfx = df.copy()\n",
    "    dfx[\"doc\"] = dfx[\"doc\"].astype(str)\n",
    "    dfx[\"dt\"] = pd.to_datetime(dfx[\"dt\"], errors=\"coerce\")\n",
    "    dfx[\"pid\"] = pd.to_numeric(dfx[\"pid\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    dfx[\"texto\"] = dfx[\"texto\"].astype(str)\n",
    "\n",
    "    # 1) remove vazios/sem letras\n",
    "    mask_nonempty = dfx[\"texto\"].str.strip().ne(\"\")\n",
    "    mask_alpha = dfx[\"texto\"].str.contains(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", regex=True)\n",
    "    dfx = dfx[mask_nonempty & mask_alpha].copy()\n",
    "\n",
    "    # 2) remove headings/legendas\n",
    "    dfx = dfx[~dfx[\"texto\"].map(is_heading)].copy()\n",
    "\n",
    "    # 3) limpeza fina + recount\n",
    "    dfx[\"texto\"] = dfx[\"texto\"].map(clean_text_unit)\n",
    "    dfx[\"n_palavras\"] = dfx[\"texto\"].map(count_words).astype(int)\n",
    "\n",
    "    # 4) fusão de curtas por (doc, dt)\n",
    "    groups = []\n",
    "    for (doc, dt), g in dfx.sort_values([\"doc\",\"pid\"]).groupby([\"doc\",\"dt\"], sort=False):\n",
    "        g2 = _fuse_short_paragraphs(g[[\"pid\",\"texto\"]], min_words=min_palavras, max_merge_span=max_merge_span)\n",
    "        g2[\"doc\"], g2[\"dt\"] = doc, dt\n",
    "        groups.append(g2)\n",
    "    if groups:\n",
    "        dfx = pd.concat(groups, ignore_index=True)\n",
    "    else:\n",
    "        # nenhum grupo → retorna vazio com colunas padrão\n",
    "        return dfx.assign(n_palavras=pd.Series(dtype=int))[[\"doc\",\"dt\",\"pid\",\"texto\",\"n_palavras\"]]\n",
    "\n",
    "    # 5) recount + mínimo final\n",
    "    dfx[\"texto\"] = dfx[\"texto\"].map(clean_text_unit)\n",
    "    dfx[\"n_palavras\"] = dfx[\"texto\"].map(count_words).astype(int)\n",
    "    dfx = dfx[dfx[\"n_palavras\"] >= min_palavras].copy()\n",
    "\n",
    "    # 6) ordena e reindexa pid por doc\n",
    "    dfx = dfx.sort_values([\"dt\",\"doc\",\"pid\"]).reset_index(drop=True)\n",
    "    dfx[\"pid\"] = dfx.groupby([\"doc\",\"dt\"]).cumcount() + 1\n",
    "    return dfx[[\"doc\",\"dt\",\"pid\",\"texto\",\"n_palavras\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f096660",
   "metadata": {},
   "source": [
    "**O que faz (principais utilitários):**\n",
    "\n",
    "* `count_words` — conta **palavras** (tokens) no texto.\n",
    "* `caps_ratio` — mede proporção de **palavras em MAIÚSCULAS** (útil para detectar títulos).\n",
    "* `is_heading` — heurística para filtrar **títulos/legendas** (ex.: começa com numeração “2.3”, contém “Figura/Tabela”, alto `caps_ratio`, sem pontuação final).\n",
    "* `clean_text_unit` — limpeza fina: remove quebras, espaços excessivos, numeração de seção.\n",
    "* `_fuse_short_paragraphs` — **funde parágrafos muito curtos** (até atingir `min_words`, limitando `max_merge_span`).\n",
    "* `limpar_paragrafos` — **pipeline** de limpeza:\n",
    "\n",
    "  1. remove vazios/sem letras,\n",
    "  2. remove headings/legendas,\n",
    "  3. limpeza fina + **reconta palavras**,\n",
    "  4. funde curtos **por (doc, dt)**,\n",
    "  5. reforça mínimo de palavras,\n",
    "  6. **reindexa `pid`** (posição do parágrafo no documento/data).\n",
    "\n",
    "**Conceitos rápidos:**\n",
    "\n",
    "* **Heurística**: regra prática que funciona “na maioria dos casos”.\n",
    "* **Reindexar `pid`**: renumerar a ordem dos parágrafos após limpeza/fusão para manter consistência.\n",
    "\n",
    "**Saída:** funções prontas para uso; nenhuma tabela imediata.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81c8b7",
   "metadata": {},
   "source": [
    "## Bloco 5 — Aplicação da limpeza e amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f04029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1079, 5) → (793, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>dt</th>\n",
       "      <th>pid</th>\n",
       "      <th>texto</th>\n",
       "      <th>n_palavras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202201</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Em 2021, o saldo do crédito ampliado ao setor ...</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202201</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>O volume de crédito do SFN alcançou R$4,7 tril...</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202201</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>O crédito livre às famílias atingiu R$ 1,5 tri...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202201</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>Em 2021, o crédito direcionado atingiu R$ 1, 9...</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202201</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>O Indicador de Custo do Crédito (ICC), que med...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>202201</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>6</td>\n",
       "      <td>A taxa média de juros das contratações finaliz...</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>202201</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>7</td>\n",
       "      <td>A inadimplência do crédito geral atingiu 2, 3%...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>202201</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>8</td>\n",
       "      <td>3. Agregados monetários Em 2021, a base monetá...</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>202201</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>9</td>\n",
       "      <td>De acordo com a Política de Revisão das Estatí...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>202201</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>i) Crédito por setor de atividade econômica : ...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      doc         dt  pid                                              texto  \\\n",
       "0  202201 2022-01-01    1  Em 2021, o saldo do crédito ampliado ao setor ...   \n",
       "1  202201 2022-01-01    2  O volume de crédito do SFN alcançou R$4,7 tril...   \n",
       "2  202201 2022-01-01    3  O crédito livre às famílias atingiu R$ 1,5 tri...   \n",
       "3  202201 2022-01-01    4  Em 2021, o crédito direcionado atingiu R$ 1, 9...   \n",
       "4  202201 2022-01-01    5  O Indicador de Custo do Crédito (ICC), que med...   \n",
       "5  202201 2022-01-01    6  A taxa média de juros das contratações finaliz...   \n",
       "6  202201 2022-01-01    7  A inadimplência do crédito geral atingiu 2, 3%...   \n",
       "7  202201 2022-01-01    8  3. Agregados monetários Em 2021, a base monetá...   \n",
       "8  202201 2022-01-01    9  De acordo com a Política de Revisão das Estatí...   \n",
       "9  202201 2022-01-01   10  i) Crédito por setor de atividade econômica : ...   \n",
       "\n",
       "   n_palavras  \n",
       "0         250  \n",
       "1         206  \n",
       "2          56  \n",
       "3         166  \n",
       "4          93  \n",
       "5         203  \n",
       "6          59  \n",
       "7         363  \n",
       "8          85  \n",
       "9         101  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# se você já tem df_paragrafos carregado:\n",
    "df_paragrafos_limpos = limpar_paragrafos(df_paragrafos, min_palavras=8, max_merge_span=3)\n",
    "\n",
    "# checagem rápida\n",
    "print(df_paragrafos.shape, \"→\", df_paragrafos_limpos.shape)\n",
    "display(df_paragrafos_limpos.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09996d2e",
   "metadata": {},
   "source": [
    "**O que faz:** aplica a limpeza completa sobre `df_paragrafos`, checa o **efeito na forma** (linhas/colunas) e exibe **amostra** (primeiras 10 linhas).\n",
    "\n",
    "**Por quê:** validar que títulos/legendas foram removidos, parágrafos curtos foram fundidos e que a estrutura (`doc`, `dt`, `pid`, `texto`, `n_palavras`) permaneceu correta.\n",
    "\n",
    "**Saída:** `df_paragrafos_limpos` (dataframe limpo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a51c7a4",
   "metadata": {},
   "source": [
    "## Instalação\n",
    "\n",
    "O que faz: instala libs para carregar modelos (transformers), datasets (datasets), métricas (sklearn) e treino (torch/accelerate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571019e",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install -U transformers datasets scikit-learn accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6daaab",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Se já existir df_paragrafos_limpos na memória, use; senão, carregue do CSV salvo no seu bloco anterior:\n",
    "if 'df_paragrafos_limpos' not in globals():\n",
    "    if Path(\"paragrafos.csv\").exists():\n",
    "        df_paragrafos_limpos = pd.read_csv(\"paragrafos.csv\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Não encontrei df_paragrafos_limpos nem 'paragrafos.csv'. Execute as células de extração/limpeza antes.\")\n",
    "\n",
    "# 2) Adicione/mescle rótulos\n",
    "if 'label' not in df_paragrafos_limpos.columns:\n",
    "    if Path(\"rotulos.csv\").exists():\n",
    "        rot = pd.read_csv(\"rotulos.csv\")  # deve ter doc, dt, pid, label\n",
    "        df = df_paragrafos_limpos.merge(rot, on=[\"doc\",\"dt\",\"pid\"], how=\"left\")\n",
    "    else:\n",
    "        # Exemplo de placeholder: TODO — preencha \"label\" manualmente (negativo/neutro/positivo OU hawkish/neutro/dovish)\n",
    "        df = df_paragrafos_limpos.copy()\n",
    "        df[\"label\"] = None  # ← Preencha e salve um CSV para rotular\n",
    "        df.to_csv(\"dataset_para_rotular.csv\", index=False)\n",
    "        raise RuntimeError(\"Criei 'dataset_para_rotular.csv'. Preencha a coluna 'label' e salve como 'rotulos.csv' (doc,dt,pid,label).\")\n",
    "else:\n",
    "    df = df_paragrafos_limpos.copy()\n",
    "\n",
    "# 3) Filtra apenas linhas rotuladas\n",
    "df = df.dropna(subset=[\"label\"]).copy()\n",
    "df[\"label\"] = df[\"label\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# 4) Escolha seu esquema de rótulos\n",
    "LABELS = [\"negativo\",\"neutro\",\"positivo\"]  # ou [\"hawkish\",\"neutro\",\"dovish\"]\n",
    "df = df[df[\"label\"].isin(LABELS)].copy()\n",
    "\n",
    "# 5) Salva dataset final (útil para reuso)\n",
    "Path(\"out\").mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(\"out/dataset_sentimento.csv\", index=False)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3bc5d",
   "metadata": {},
   "source": [
    "# 2) DeB3RTA\n",
    "\n",
    "> Seção (markdown) que indica o próximo passo do notebook: uso do modelo **DeB3RTa** (variante do DeBERTa) para tarefas de linguagem.\n",
    "> Dica: aqui normalmente entram **tokenização** (quebrar texto em subpalavras), **preparo de dataset** e **inferência/treino**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da12604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, precision_recall_fscore_support\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# === Config DeB3RTa ===\n",
    "MODEL_ID = \"higopires/DeB3RTa-small\"  # ou ...-base\n",
    "OUTPUT_DIR = \"out/deb3rta\"\n",
    "MAX_LEN = 256\n",
    "CHUNK_STRIDE = 50\n",
    "LABELS = [\"negativo\",\"neutro\",\"positivo\"]\n",
    "label2id = {lab:i for i,lab in enumerate(LABELS)}\n",
    "id2label = {i:lab for lab,i in label2id.items()}\n",
    "\n",
    "# === Carrega dataset limpo ===\n",
    "df = pd.read_csv(\"out/dataset_sentimento.csv\")\n",
    "df = df[df[\"label\"].isin(LABELS)].copy()\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "train_df, temp_df = train_split = train_test_split(df, test_size=0.3, stratify=df[\"label_id\"], random_state=SEED)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label_id\"], random_state=SEED)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "def chunk_encode(text: str, doc_id: str, label_id: int):\n",
    "    enc = tok(text, truncation=False, padding=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "    ids = enc[\"input_ids\"]\n",
    "    chunks = []\n",
    "    if len(ids) <= MAX_LEN:\n",
    "        pad = tok(text, truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "        chunks.append({\"input_ids\": pad[\"input_ids\"], \"attention_mask\": pad[\"attention_mask\"],\n",
    "                       \"labels\": label_id, \"doc_id\": doc_id})\n",
    "    else:\n",
    "        start = 0\n",
    "        while start < len(ids):\n",
    "            end = min(start + MAX_LEN, len(ids))\n",
    "            window = ids[start:end]\n",
    "            attn = [1]*len(window)\n",
    "            if len(window) < MAX_LEN:\n",
    "                pad = [tok.pad_token_id]*(MAX_LEN-len(window))\n",
    "                window += pad\n",
    "                attn += [0]*len(pad)\n",
    "            chunks.append({\"input_ids\": window, \"attention_mask\": attn, \"labels\": label_id, \"doc_id\": doc_id})\n",
    "            if end == len(ids): break\n",
    "            start = end - CHUNK_STRIDE\n",
    "    return chunks\n",
    "\n",
    "def build_ds(df_part):\n",
    "    rows = []\n",
    "    for r in df_part.itertuples():\n",
    "        rows.extend(chunk_encode(r.texto, f\"{r.doc}|{r.dt}|{r.pid}\", int(r.label_id)))\n",
    "    data = {k:[d[k] for d in rows] for k in rows[0].keys()}\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "train_ds = build_ds(train_df); valid_ds = build_ds(valid_df); test_ds = build_ds(test_df)\n",
    "ds = DatasetDict(train=train_ds, validation=valid_ds, test=test_ds)\n",
    "\n",
    "# Pesos de classe\n",
    "counts = np.bincount(train_df[\"label_id\"], minlength=len(LABELS))\n",
    "w = (counts.sum() / np.maximum(counts, 1)); w = w / w.mean()\n",
    "w_t = torch.tensor(w, dtype=torch.float)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID, num_labels=len(LABELS), id2label=id2label, label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# (Opcional) Reinicialize as últimas camadas para adaptar mais rápido (layer re-init)\n",
    "# k = 2\n",
    "# encoder = model.deberta.encoder\n",
    "# for i in range(1, k+1):\n",
    "#     encoder.layer[-i].apply(model._init_weights)\n",
    "\n",
    "orig_forward = model.forward\n",
    "def forward_weighted(**kwargs):\n",
    "    out = orig_forward(**kwargs)\n",
    "    if \"labels\" in kwargs:\n",
    "        logits = out.logits\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=w_t.to(logits.device))\n",
    "        loss = loss_fct(logits.view(-1, len(LABELS)), kwargs[\"labels\"].view(-1))\n",
    "        return type(out)(loss=loss, logits=logits, hidden_states=out.hidden_states, attentions=out.attentions)\n",
    "    return out\n",
    "model.forward = forward_weighted\n",
    "\n",
    "def metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1m = f1_score(labels, preds, average=\"macro\")\n",
    "    p, r, f1w, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    return {\"acc\": acc, \"f1_macro\": f1m, \"prec_w\": p, \"recall_w\": r}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=4,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200, save_steps=200, logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\", greater_is_better=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=args, train_dataset=ds[\"train\"], eval_dataset=ds[\"validation\"],\n",
    "    compute_metrics=metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Avaliação\n",
    "pred_val = trainer.predict(ds[\"validation\"])\n",
    "pred_tst = trainer.predict(ds[\"test\"])\n",
    "for name, pred in [(\"VAL\", pred_val), (\"TEST\", pred_tst)]:\n",
    "    y_true = pred.label_ids\n",
    "    y_hat  = pred.predictions.argmax(axis=1)\n",
    "    print(f\"\\n=== DeB3RTa — {name} ===\")\n",
    "    print(classification_report(y_true, y_hat, target_names=LABELS, digits=4))\n",
    "    print(\"Matriz de confusão:\\n\", confusion_matrix(y_true, y_hat))\n",
    "\n",
    "# Salva métricas simples\n",
    "deb3_metrics = {\n",
    "    \"val\": {\"acc\": float(accuracy_score(pred_val.label_ids, pred_val.predictions.argmax(axis=1))),\n",
    "            \"f1_macro\": float(f1_score(pred_val.label_ids, pred_val.predictions.argmax(axis=1), average=\"macro\"))},\n",
    "    \"test\": {\"acc\": float(accuracy_score(pred_tst.label_ids, pred_tst.predictions.argmax(axis=1))),\n",
    "             \"f1_macro\": float(f1_score(pred_tst.label_ids, pred_tst.predictions.argmax(axis=1), average=\"macro\"))}\n",
    "}\n",
    "import json, pathlib\n",
    "pathlib.Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "json.dump(deb3_metrics, open(os.path.join(OUTPUT_DIR, \"metrics.json\"), \"w\"), indent=2)\n",
    "\n",
    "print(\"\\nMelhor checkpoint salvo em:\", trainer.state.best_model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc11fe3",
   "metadata": {},
   "source": [
    "# 3) FinBERT-PT-BR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, precision_recall_fscore_support\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# === Config FinBERT (substitua por outro ID se preferir) ===\n",
    "MODEL_ID = \"lucas-leme/FinBERT-PT-BR\"\n",
    "OUTPUT_DIR = \"out/finbert_pt_br\"\n",
    "MAX_LEN = 256\n",
    "CHUNK_STRIDE = 50\n",
    "LABELS = [\"negativo\",\"neutro\",\"positivo\"]  # deve bater com o bloco (1)\n",
    "label2id = {lab:i for i,lab in enumerate(LABELS)}\n",
    "id2label = {i:lab for lab,i in label2id.items()}\n",
    "\n",
    "# === Carrega dataset limpo ===\n",
    "df = pd.read_csv(\"out/dataset_sentimento.csv\")\n",
    "df = df[df[\"label\"].isin(LABELS)].copy()\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df[\"label_id\"], random_state=SEED)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label_id\"], random_state=SEED)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "def chunk_encode(text: str, doc_id: str, label_id: int):\n",
    "    enc = tok(text, truncation=False, padding=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "    ids = enc[\"input_ids\"]\n",
    "    chunks = []\n",
    "    if len(ids) <= MAX_LEN:\n",
    "        pad = tok(text, truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "        chunks.append({\"input_ids\": pad[\"input_ids\"], \"attention_mask\": pad[\"attention_mask\"],\n",
    "                       \"labels\": label_id, \"doc_id\": doc_id})\n",
    "    else:\n",
    "        start = 0\n",
    "        while start < len(ids):\n",
    "            end = min(start + MAX_LEN, len(ids))\n",
    "            window = ids[start:end]\n",
    "            attn = [1]*len(window)\n",
    "            if len(window) < MAX_LEN:\n",
    "                pad = [tok.pad_token_id]*(MAX_LEN-len(window))\n",
    "                window += pad\n",
    "                attn += [0]*len(pad)\n",
    "            chunks.append({\"input_ids\": window, \"attention_mask\": attn, \"labels\": label_id, \"doc_id\": doc_id})\n",
    "            if end == len(ids): break\n",
    "            start = end - CHUNK_STRIDE\n",
    "    return chunks\n",
    "\n",
    "def build_ds(df_part):\n",
    "    rows = []\n",
    "    for r in df_part.itertuples():\n",
    "        rows.extend(chunk_encode(r.texto, f\"{r.doc}|{r.dt}|{r.pid}\", int(r.label_id)))\n",
    "    data = {k:[d[k] for d in rows] for k in rows[0].keys()}\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "train_ds = build_ds(train_df); valid_ds = build_ds(valid_df); test_ds = build_ds(test_df)\n",
    "ds = DatasetDict(train=train_ds, validation=valid_ds, test=test_ds)\n",
    "\n",
    "# Pesos de classe (corrigem desbalanceamento)\n",
    "counts = np.bincount(train_df[\"label_id\"], minlength=len(LABELS))\n",
    "w = (counts.sum() / np.maximum(counts, 1)); w = w / w.mean()\n",
    "w_t = torch.tensor(w, dtype=torch.float)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID, num_labels=len(LABELS), id2label=id2label, label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Substitui a loss padrão por uma CrossEntropy com pesos\n",
    "orig_forward = model.forward\n",
    "def forward_weighted(**kwargs):\n",
    "    out = orig_forward(**kwargs)\n",
    "    if \"labels\" in kwargs:\n",
    "        logits = out.logits\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=w_t.to(logits.device))\n",
    "        loss = loss_fct(logits.view(-1, len(LABELS)), kwargs[\"labels\"].view(-1))\n",
    "        return type(out)(loss=loss, logits=logits, hidden_states=out.hidden_states, attentions=out.attentions)\n",
    "    return out\n",
    "model.forward = forward_weighted\n",
    "\n",
    "def metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1m = f1_score(labels, preds, average=\"macro\")\n",
    "    p, r, f1w, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\", zero_division=0)\n",
    "    return {\"acc\": acc, \"f1_macro\": f1m, \"prec_w\": p, \"recall_w\": r}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=4,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200, save_steps=200, logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\", greater_is_better=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=args, train_dataset=ds[\"train\"], eval_dataset=ds[\"validation\"],\n",
    "    compute_metrics=metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Avaliação\n",
    "pred_val = trainer.predict(ds[\"validation\"])\n",
    "pred_tst = trainer.predict(ds[\"test\"])\n",
    "for name, pred in [(\"VAL\", pred_val), (\"TEST\", pred_tst)]:\n",
    "    y_true = pred.label_ids\n",
    "    y_hat  = pred.predictions.argmax(axis=1)\n",
    "    print(f\"\\n=== FinBERT — {name} ===\")\n",
    "    print(classification_report(y_true, y_hat, target_names=LABELS, digits=4))\n",
    "    print(\"Matriz de confusão:\\n\", confusion_matrix(y_true, y_hat))\n",
    "\n",
    "# Salva métricas simples para comparar depois\n",
    "finbert_metrics = {\n",
    "    \"val\": {\"acc\": float(accuracy_score(pred_val.label_ids, pred_val.predictions.argmax(axis=1))),\n",
    "            \"f1_macro\": float(f1_score(pred_val.label_ids, pred_val.predictions.argmax(axis=1), average=\"macro\"))},\n",
    "    \"test\": {\"acc\": float(accuracy_score(pred_tst.label_ids, pred_tst.predictions.argmax(axis=1))),\n",
    "             \"f1_macro\": float(f1_score(pred_tst.label_ids, pred_tst.predictions.argmax(axis=1), average=\"macro\"))}\n",
    "}\n",
    "import json, pathlib\n",
    "pathlib.Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "json.dump(finbert_metrics, open(os.path.join(OUTPUT_DIR, \"metrics.json\"), \"w\"), indent=2)\n",
    "\n",
    "print(\"\\nMelhor checkpoint salvo em:\", trainer.state.best_model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "paths = {\n",
    "    \"FinBERT-PT-BR\": \"out/finbert_pt_br/metrics.json\",\n",
    "    \"DeB3RTa\": \"out/deb3rta/metrics.json\"\n",
    "}\n",
    "rows = []\n",
    "for name, path in paths.items():\n",
    "    if Path(path).exists():\n",
    "        m = json.load(open(path))\n",
    "        rows.append({\"modelo\": name,\n",
    "                     \"val_acc\": m[\"val\"][\"acc\"], \"val_f1_macro\": m[\"val\"][\"f1_macro\"],\n",
    "                     \"test_acc\": m[\"test\"][\"acc\"], \"test_f1_macro\": m[\"test\"][\"f1_macro\"]})\n",
    "df_cmp = pd.DataFrame(rows).sort_values([\"test_f1_macro\",\"test_acc\"], ascending=False)\n",
    "df_cmp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
